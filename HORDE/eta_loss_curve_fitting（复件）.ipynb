{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q_error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "# dataset='census13'\n",
    "# version='original'\n",
    "\n",
    "plt.rcParams['axes.linewidth']=3\n",
    "plt.rcParams['font.size']=18\n",
    "plt.rcParams['lines.linewidth']=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_error_distribution(dataset,version):\n",
    "    plt.rcParams['axes.linewidth']=3\n",
    "    plt.rcParams['font.size']=22\n",
    "    plt.rcParams['lines.linewidth']=4\n",
    "    \n",
    "    estimators=['naru','mscn','deepdb']\n",
    "    markers=['d','^',\".\"]\n",
    "    colors=['']\n",
    "    addr=\"./lecarb/estimator/mine/learning_model_prediction/\"+dataset+\"_\"+version+\".pkl\"\n",
    "    with open(addr, 'rb') as f:\n",
    "        [_,__,data_length] = pickle.load(f)\n",
    "    \n",
    "    plt.subplots(1,2,figsize=(12,4))\n",
    "    plt.subplots_adjust(left=0,right=1,top=1,bottom=0,wspace=0.4,hspace=0.3)\n",
    "    for th in range(len(estimators)):\n",
    "        estimator=estimators[th]\n",
    "        result_addr=\"./lecarb/estimator/predict_result/\"+estimator+\"_model_prediction/test_\"+dataset+\"_\"+version+\".pkl\"\n",
    "        with open(result_addr, 'rb') as f:\n",
    "            [pres,labels] = pickle.load(f)\n",
    "        x=[]\n",
    "        q_error=[]\n",
    "        absolute_error=[]\n",
    "        for i in range(len(pres)):\n",
    "            x.append(labels[i]/data_length)\n",
    "            absolute_error.append(abs(labels[i]-pres[i]))\n",
    "            if pres[i]==0 and labels[i]==0:\n",
    "                q_error.append(1)\n",
    "            elif pres[i]==0:\n",
    "                q_error.append(labels[i])\n",
    "            elif labels[i]==0:\n",
    "                q_error.append(pres[i])\n",
    "            else:\n",
    "                q_error.append(max(labels[i]/pres[i],pres[i]/labels[i]))\n",
    "        plt.subplot(121)\n",
    "        plt.scatter(x,q_error,label=estimator,marker=markers[th])\n",
    "        plt.xlabel('cardinality rate')\n",
    "        plt.ylabel('q_error')\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        plt.scatter(x,absolute_error,label=estimator,marker=markers[th])\n",
    "        plt.xlabel('cardinality rate')\n",
    "        plt.ylabel('absolute error')\n",
    "    plt.legend(bbox_to_anchor=(1.15,0),loc=3,borderaxespad=0)\n",
    "    plt.savefig(\"q_error_dist_\"+dataset+\"_\"+version+\".png\",dpi=600,format='png',bbox_inches='tight')    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './lecarb/estimator/mine/learning_model_prediction/census13_original.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5fd92bccde8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq_error_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'census13'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'original'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-0e5a79780643>\u001b[0m in \u001b[0;36mq_error_distribution\u001b[0;34m(dataset, version)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0maddr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./lecarb/estimator/mine/learning_model_prediction/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_length\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './lecarb/estimator/mine/learning_model_prediction/census13_original.pkl'"
     ]
    }
   ],
   "source": [
    "q_error_distribution('census13','original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_error_distribution('forest10','original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_error_distribution('power7','original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_error_distribution('dmv11','original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('just mine-AR_tree_inference census13 original;just mine-AR_tree_inference forest10 original;just mine-AR_tree_inference power7 original;just mine-AR_tree_inference dmv11 original')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "# dataset='census13'\n",
    "# version='original'\n",
    "\n",
    "plt.rcParams['axes.linewidth']=3\n",
    "plt.rcParams['font.size']=18\n",
    "plt.rcParams['lines.linewidth']=3\n",
    "\n",
    "\n",
    "def learning_performance(dataset,version):\n",
    "#     plt.rcParams=['axes.linewidth']=3\n",
    "#     plt.rcParams=['font.size']=18\n",
    "#     plt.rcParams=['lines.linewidth']=3\n",
    "    result_addr=\"./lecarb/estimator/mine/tree_inference_result/valid_\"+dataset+\"_\"+version+\".pkl\"\n",
    "    with open(result_addr, 'rb') as f:\n",
    "        [inference_result,inference_time] = pickle.load(f)\n",
    "\n",
    "    addr=\"./lecarb/estimator/mine/learning_model_prediction/\"+dataset+\"_\"+version+\".pkl\"\n",
    "    with open(addr, 'rb') as f:\n",
    "        [prediction,label,data_length] = pickle.load(f)\n",
    "\n",
    "    prediction=prediction.cpu().detach().numpy()\n",
    "    count_prediction=[float(i) for i in prediction]\n",
    "\n",
    "    prediction_values=sorted(Counter(count_prediction).keys(),key=lambda x:x,reverse=False)\n",
    "    # print(prediction_values[:10])\n",
    "    # print(len(prediction_values))\n",
    "    \n",
    "    q_error=[]\n",
    "    MAE=[]\n",
    "    MAPE=[]\n",
    "    for i in range(len(prediction)):\n",
    "        p=int(prediction[i]*data_length)\n",
    "        l=int(label[i]*data_length)\n",
    "        if p==0 and l==0:\n",
    "            q_error.append(1)\n",
    "        elif p==0:\n",
    "            q_error.append(l)\n",
    "        elif l==0:\n",
    "            q_error.append(p)\n",
    "        else:\n",
    "            q_error.append(max(l/p,p/l))\n",
    "        MAE.append(abs(l-p))\n",
    "        if l!=0:\n",
    "            MAPE.append(abs(l-p)/l)\n",
    "        else:\n",
    "            MAPE.append(p)\n",
    "            \n",
    "    \n",
    "    plt.xlabel('cardinality')\n",
    "    plt.ylabel('learning model result q_error')\n",
    "    plt.scatter(label,q_error,label='q_error')\n",
    "    plt.show()\n",
    "    \n",
    "    log_label=[]\n",
    "    log_q_error=[]\n",
    "    for i in range(len(q_error)):\n",
    "        if label[i]>=0.05:\n",
    "            log_label.append(label[i])\n",
    "            log_q_error.append(math.log(q_error[i]))\n",
    "\n",
    "    ymin=np.min(log_q_error)\n",
    "    ymax=np.max(log_q_error)\n",
    "#     plt.yscale('log')\n",
    "    print(ymin,ymax)\n",
    "    plt.xlabel('cardinality')\n",
    "    plt.ylabel('learning model result log q_error')\n",
    "    plt.ylim(ymin,ymax)\n",
    "    \n",
    "    plt.yscale('log')\n",
    "    plt.scatter(log_label,log_q_error,label='log q_error',marker='o')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    plt.xlabel('cardinality'+' (tuple number:'+str(data_length)+\")\")\n",
    "    plt.ylabel('learning model result MAE')\n",
    "    plt.scatter(label,MAE,label='MAE')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.xlabel('cardinality')\n",
    "    plt.ylabel('learning model result MAPE')\n",
    "    plt.scatter(label,MAPE,label='MAPE')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习模型(不包括树)本身的性能\n",
    "其中1，2，3图分别对应着q_error,AE(absolute error),APE(absolute percentage error)\n",
    "\n",
    "假设p : prediction, l : label\n",
    "\n",
    "q_error=max($\\frac{l}{p}$,$\\frac{p}{l}$)\n",
    "\n",
    "AE=$|l-p|$\n",
    "\n",
    "APE=$\\frac{|l-p|}{l}$\n",
    "\n",
    "例如：第1/2/3幅图中（0.1，100）点表示cardinality label为总体10%的query其被学习模型预测的结果的\n",
    "      q_error/AE/APE 为100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_performance('census13','original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_performance('forest10','original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_performance('power7','original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_performance('dmv11','original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# dataset='census1a3'\n",
    "# version='original'\n",
    "\n",
    "\n",
    "def drawing(dataset,version,rate):\n",
    "    result_addr=\"./lecarb/estimator/mine/tree_inference_result/valid_\"+dataset+\"_\"+version+\".pkl\"\n",
    "    with open(result_addr, 'rb') as f:\n",
    "        [inference_result,inference_time] = pickle.load(f)\n",
    "\n",
    "    addr=\"./lecarb/estimator/mine/learning_model_prediction/valid_\"+dataset+\"_\"+version+\".pkl\"\n",
    "    with open(addr, 'rb') as f:\n",
    "        [prediction,label,data_length] = pickle.load(f)\n",
    "        \n",
    "       \n",
    "    prediction=prediction.cpu().detach().numpy()\n",
    "    count_prediction=[float(i) for i in prediction]\n",
    "\n",
    "    prediction_values=sorted(Counter(count_prediction).keys(),key=lambda x:x,reverse=False)\n",
    "    # print(prediction_values[:10])\n",
    "    # print(len(prediction_values))\n",
    "    label=np.around(label*data_length)\n",
    "    label=[i[0] for i in label]\n",
    "    \n",
    "\n",
    "    loss=[] # 0.5*t+0.5*q_error\n",
    "    t=[]\n",
    "    mean_qerror=[]\n",
    "    for threshold in prediction_values:\n",
    "        add_time=0\n",
    "        q_error=[]\n",
    "        for i in range(len(prediction)):\n",
    "            if prediction[i]<=threshold:\n",
    "                q_error.append(1)\n",
    "                add_time+=inference_time[i]\n",
    "            else:\n",
    "                p=np.around(prediction[i]*data_length)[0]\n",
    "                \n",
    "                \n",
    "                l=label[i]\n",
    "                if p==0 and l==0:\n",
    "                    q_error.append(1)\n",
    "                elif p==0:\n",
    "                    q_error.append(l)\n",
    "                elif l==0:\n",
    "                    q_error.append(p)\n",
    "                else:\n",
    "                    q_error.append(max(p/l,l/p))\n",
    "        add_time=add_time*1000/10000\n",
    "        #add_time ms/query\n",
    "#         loss.append(add_time*rate+(1-rate)*np.mean(q_error))\n",
    "        loss.append(add_time+np.mean(q_error))\n",
    "        t.append(add_time)\n",
    "        mean_qerror.append(np.mean(q_error))\n",
    "    print(\"best eta:\",prediction_values[loss.index(min(loss))])\n",
    "    \n",
    "    threshold=prediction_values[loss.index(min(loss))]\n",
    "    add_time=0\n",
    "    q_error=[]\n",
    "    turn_to_precise=0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i]<=threshold:\n",
    "            turn_to_precise+=1\n",
    "            q_error.append(1)\n",
    "            add_time+=inference_time[i]\n",
    "        else:\n",
    "            p=np.around(prediction[i]*data_length)[0]\n",
    "            l=label[i]\n",
    "            if p==0 and l==0:\n",
    "                q_error.append(1)\n",
    "            elif p==0:\n",
    "                q_error.append(l)\n",
    "            elif l==0:\n",
    "                q_error.append(p)\n",
    "            else:\n",
    "                q_error.append(max(p/l,l/p))\n",
    "    add_time=add_time*1000/10000\n",
    "    print(\"max:\",np.max(q_error),'99th:',np.percentile(q_error,99),'95th:',np.percentile(q_error,95),'90th:',np.percentile(q_error,90),'75th:',np.percentile(q_error,75),'50th:',np.percentile(q_error,50),'25th:',np.percentile(q_error,25),'mean:',np.mean(q_error))\n",
    "    print(\"average time:\",add_time,\"ms/query\")\n",
    "    print(\"turn_to_precise\",turn_to_precise)\n",
    "    \n",
    "    plt.xlabel('eta')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(prediction_values,loss)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.xlabel('eta')\n",
    "    plt.ylabel('incremented time of using tree')\n",
    "    plt.plot(prediction_values,t)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.xlabel('eta')\n",
    "    plt.ylabel('mean q_error after using tree')\n",
    "    plt.plot(prediction_values,mean_qerror)\n",
    "    plt.show()\n",
    "    \n",
    "    return prediction_values,loss,t,mean_qerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit,minimize\n",
    "import matplotlib.pyplot as mpl\n",
    "\n",
    "from sympy import symbols, diff\n",
    "from sympy.functions import exp\n",
    "\n",
    "def func1_for_diff(a,x0,sigma):\n",
    "    x=symbols('x')\n",
    "    f=a*exp(-(x-x0)**2/(2*sigma**2))\n",
    "    derivative_f = diff(f, x)\n",
    "    return f,derivative_f\n",
    "\n",
    "def func2_for_diff(a,x0,sigma):\n",
    "    x=symbols('x')\n",
    "    f=a*exp(-(x-x0)**2/(2*sigma**2))+1\n",
    "    derivative_f = diff(f, x)\n",
    "    return f,derivative_f\n",
    "\n",
    "# Let's create a function to model and create data\n",
    "def func1(x, a, x0, sigma):\n",
    "    return a*np.exp(-(x-x0)**2/(2*sigma**2))\n",
    "\n",
    "def func2(x, a, x0, sigma):\n",
    "    return a*np.exp(-(x-x0)**2/(2*sigma**2))+1\n",
    "\n",
    "def exponen(x,a,b,lamb):\n",
    "    if type(x)==type([]):\n",
    "        x=np.array(x)\n",
    "    return a-b*np.exp(-(lamb*x))\n",
    "\n",
    "def exponen_for_diff(a,b,lamb):\n",
    "    x=symbols('x')\n",
    "    f=a-b*exp(-(lamb*x))\n",
    "    derivative_f = diff(f, x)\n",
    "    return f,derivative_f\n",
    "\n",
    "def fit2(x,y,fit_type):\n",
    "    plt.plot(x, y, c='k', label='data')\n",
    "    plt.scatter(x, y)\n",
    "    if fit_type==1:\n",
    "        popt, pcov = curve_fit(exponen, x, y,maxfev=1000000)\n",
    "        ym = exponen(x, popt[0], popt[1], popt[2])\n",
    "    elif fit_type==2:\n",
    "        popt, pcov = curve_fit(func2, x, y,maxfev=1000000)\n",
    "        ym = func2(x, popt[0], popt[1], popt[2])\n",
    "    else:\n",
    "        print(\"wrong type\")\n",
    "        return\n",
    "        \n",
    "    #popt returns the best fit values for parameters of the given model (func)\n",
    "\n",
    "    plt.plot(x, ym, c='r', label='fit')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return popt\n",
    "\n",
    "def exponent_func(a,b,lamb):\n",
    "    f=lambda x:a-b*np.exp(-(lamb*x))\n",
    "    return f\n",
    "\n",
    "def func2_func(a,x0,sigma):\n",
    "    f=lambda x:a*np.exp(-(x-x0)**2/(2*sigma**2))+1\n",
    "    return f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_find_best_eta(prediction_values,t,mean_qerror,rate):\n",
    "    x=symbols('x')\n",
    "    popt1=fit2(prediction_values,t,fit_type=1)\n",
    "    popt2=fit2(prediction_values,mean_qerror,fit_type=2)\n",
    "    f1,_=exponen_for_diff(popt1[0],popt1[1],popt1[2])\n",
    "    print(\"a:\",popt1[0])\n",
    "    print('b:',popt1[1])\n",
    "    print('lambda:',popt1[2])\n",
    "    f2,_=func2_for_diff(popt2[0],popt2[1],popt2[2])\n",
    "    print(\"c:\",popt2[0])\n",
    "    print('x0:',popt2[1])\n",
    "    print('sigma:',popt2[2])\n",
    "#     f=rate*f1+(1-rate)*f2\n",
    "    f=f1+f2\n",
    "    \n",
    "    print(\"time\",f1)\n",
    "    print(\"mean q error\",f2)\n",
    "    print(f)\n",
    "    print(\"-----------------\")\n",
    "    \n",
    "    func_f1=exponent_func(popt1[0],popt1[1],popt1[2])\n",
    "    func_f2=func2_func(popt2[0],popt2[1],popt2[2])\n",
    "#     func_f=lambda x:rate*func_f1(x)+(1-rate)*func_f2(x)\n",
    "    func_f=lambda x:func_f1(x)+func_f2(x)\n",
    "    result=minimize(func_f,x0=[0.5],method='SLSQP',bounds=[(0,1)])\n",
    "    \n",
    "    \n",
    "    loss=[]\n",
    "    for i in range(len(t)):\n",
    "        loss.append(t[i]+mean_qerror[i])\n",
    "    plt.scatter(prediction_values,loss,label='loss')\n",
    "    \n",
    "    a=[i/10000 for i in range(0,10000)]\n",
    "    b=[f.subs(x,i) for i in a]\n",
    "    plt.plot(a,b,color='r',label='fit')\n",
    "    best_eta=result['x'][0]\n",
    "    print(\"best eta:\",best_eta)\n",
    "#     print(\"loss:\",f.subs(x,best_eta))\n",
    "#     print(\"time:\",f1.subs(x,best_eta))\n",
    "#     print(\"mean_qerror:\",f2.subs(x,best_eta))\n",
    "    plt.scatter(best_eta,f.subs(x,best_eta),color='green')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return best_eta,f.subs(x,best_eta),a,b\n",
    "\n",
    "\n",
    "def test_for_best_eta(dataset,version,best_eta):\n",
    "    result_addr=\"./lecarb/estimator/mine/tree_inference_result/\"+dataset+\"_\"+version+\".pkl\"\n",
    "    with open(result_addr, 'rb') as f:\n",
    "        [inference_result,inference_time] = pickle.load(f)\n",
    "\n",
    "    addr=\"./lecarb/estimator/mine/learning_model_prediction/test_\"+dataset+\"_\"+version+\".pkl\"\n",
    "    with open(addr, 'rb') as f:\n",
    "        [prediction,label,data_length] = pickle.load(f)\n",
    "        \n",
    "    label=np.around(label*data_length)\n",
    "    label=[i[0] for i in label]\n",
    "    \n",
    "#     for i in range(100):\n",
    "#         print(label[i],inference_result[i])\n",
    "#     return\n",
    "    \n",
    "    prediction=prediction.cpu().detach().numpy()\n",
    "    add_time=0\n",
    "    q_error=[]\n",
    "    turn_to_precise=0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i]<=best_eta:\n",
    "            q_error.append(1)\n",
    "            turn_to_precise+=1\n",
    "            add_time+=inference_time[i]\n",
    "        else:\n",
    "            p=np.around(prediction[i]*data_length)[0]\n",
    "            l=label[i]\n",
    "            if p==0 and l==0:\n",
    "                q_error.append(1)\n",
    "            elif p==0:\n",
    "                q_error.append(l)\n",
    "            elif l==0:\n",
    "                q_error.append(p)\n",
    "            else:\n",
    "                q_error.append(max(p/l,l/p))\n",
    "    add_time=add_time*1000/10000\n",
    "    print(\"max:\",np.max(q_error),'99th:',np.percentile(q_error,99),'95th:',np.percentile(q_error,95),'90th:',np.percentile(q_error,90),'75th:',np.percentile(q_error,75),'50th:',np.percentile(q_error,50),'25th:',np.percentile(q_error,25),'mean:',np.mean(q_error))\n",
    "    print(\"average incremental time:\",add_time,\"ms/query\")\n",
    "    print(\"turn_to_precise\",turn_to_precise)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eta-loss/time/mean q_error 曲线\n",
    "\n",
    "三幅图中的eta表达含义是一样的：例如，eta取值为0.2时，表示学习模型预测值<=0.2时相信树结构，>0.2时相信学习模型\n",
    "\n",
    "t : time\n",
    "\n",
    "mqe : mean q_error\n",
    "\n",
    "$loss=t*rate+mqe*(1-rate)$\n",
    "\n",
    "三幅图分别含义：\n",
    "\n",
    "eta-loss图中（0.2，1.25）表示，如果eta取值为0.2，则学习模型+树的loss为1.25   \n",
    "\n",
    "eta-time图中表示树结构所增加的时间，例如（0.2，1.5）表示，当eta为0.2时，使用树结构的所有query（不包含只使用学习模型的部分）平均下来每条query要多花费1.5ms\n",
    "\n",
    "eta-mean q_error图中，（0.2，1.5）表示当eta为0.2时，所有query(包括只使用学习模型以及使用学习模型+树）的平均q_error为1.5\n",
    "\n",
    "这里的best\\_eta1是直接遍历eta找到的.\n",
    "\n",
    "-----------------------------------------------------------------------------------\n",
    "\n",
    "接着利用curve_fit函数对于eta-t, eta-mqe, eta-loss 曲线进行拟合.(原理：非线性最小二乘曲线拟合)\n",
    "\n",
    "其中\n",
    "eta-t的初始点为(0,0),因此利用函数$f(x) = a-be^{-\\lambda x}$ 进行拟合\n",
    "\n",
    "eta-mqe从一个较高的点出发,随着eta的增大逐渐靠近并最终与直线y=1相交,利用函数$g(x)=1+ae^{-\\frac{(x-x_{0})^{2}}{2\\sigma^{2}}}$进行拟合\n",
    "\n",
    "最终,eta-loss曲线拟合函数为w(x)=0.5*f(x)+0.5*g(x)\n",
    "\n",
    "在此基础上利用scipy库中的minimize函数进行函数最小值点(best\\_eta,best\\_loss)的求解(原理：SLSQP，SequentialLeastSquaresProgramming,序列最小二乘法),并测得在该$eta$值的情况时模型总体性能(q_error,time等)\n",
    "\n",
    "这里的best\\_eta2是通过函数拟合并求最小值点得到的\n",
    "\n",
    "max,99th,95.... : 这些指标是在best\\_eta2的情况下,模型的q\\_error指标\n",
    "\n",
    "average incremental time : 指的是在best\\_eta2的情况下树结构平均所增加的inference时间\n",
    "\n",
    "turn_to_precise : 指的是在best\\_eta2的情况下10000条query中有多少条query是使用了树结构的\n",
    "\n",
    "------------------------------------------------------------------------------------\n",
    "\n",
    "可以看出best_eta1, best_eta2很接近,表明拟合效果较好."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# census13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_values1,loss1,t1,mean_qerror1=drawing('census13','original',0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eta1,best_loss1,fit_x1,fit_y1=fit_and_find_best_eta(prediction_values1,t1,mean_qerror1,rate=0.5)\n",
    "test_for_best_eta(\"census13\",'original',best_eta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forest10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_values2,loss2,t2,mean_qerror2=drawing('forest10','original',0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eta2,best_loss2,fit_x2,fit_y2=fit_and_find_best_eta(prediction_values2,t2,mean_qerror2,rate=0.5)\n",
    "test_for_best_eta(\"forest10\",'original',best_eta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# power7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_values3,loss3,t3,mean_qerror3=drawing('power7','original',0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eta3,best_loss3,fit_x3,fit_y3=fit_and_find_best_eta(prediction_values3,t3,mean_qerror3,rate=0.5)\n",
    "test_for_best_eta(\"power7\",'original',best_eta3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dmv11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_values4,loss4,t4,mean_qerror4=drawing('dmv11','original',0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eta4,best_loss4,fit_x4,fit_y4=fit_and_find_best_eta(prediction_values4,t4,mean_qerror4,rate=0.5)\n",
    "test_for_best_eta(\"dmv11\",'original',best_eta4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(1,4,figsize=(32,4))\n",
    "fig.subplots_adjust(left=0,right=1,top=1,bottom=0,wspace=0.4,hspace=0.3)\n",
    "ax1,ax2,ax3,ax4=axes.flat\n",
    "ax1.set_title('census13')\n",
    "ax2.set_title('forest10')\n",
    "ax3.set_title('power7')\n",
    "ax4.set_title('dmv11')\n",
    "\n",
    "ax1.scatter(prediction_values1,loss1,label='eta-loss')\n",
    "ax1.plot(fit_x1,fit_y1,label='fit curve')\n",
    "ax1.scatter(best_eta1,best_loss1)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.scatter(prediction_values2,loss2)\n",
    "ax2.plot(fit_x2,fit_y2)\n",
    "ax2.scatter(best_eta2,best_loss2)\n",
    "\n",
    "ax3.scatter(prediction_values3,loss3)\n",
    "ax3.plot(fit_x3,fit_y3)\n",
    "ax3.scatter(best_eta3,best_loss3)\n",
    "\n",
    "ax4.scatter(prediction_values4,loss4)\n",
    "ax4.plot(fit_x4,fit_y4)\n",
    "ax4.scatter(best_eta4,best_loss4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
